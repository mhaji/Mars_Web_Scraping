{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mission to Mars\n",
    "\n",
    "# ![mission_to_mars](Images/mission_to_mars.jpg)\n",
    "\n",
    "# In this assignment, you will build a web application that scrapes various websites for data related to the Mission to Mars and displays the information in a single HTML page. The following outlines what you need to do.\n",
    "\n",
    "# ## Step 1 - Scraping\n",
    "\n",
    "# Complete your initial scraping using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter.\n",
    "\n",
    "# * Create a Jupyter Notebook file called `mission_to_mars.ipynb` and use this to complete all of your scraping and analysis tasks. The following outlines what you need to scrape.\n",
    "\n",
    "# ### NASA Mars News\n",
    "\n",
    "# * Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later.\n",
    "\n",
    "# ```python\n",
    "# # Example:\n",
    "# news_title = \"NASA's Next Mars Mission to Investigate Interior of Red Planet\"\n",
    "\n",
    "# news_p = \"Preparation of NASA's next spacecraft to Mars, InSight, has ramped up this summer, on course for launch next May from Vandenberg Air Force Base in central California -- the first interplanetary launch in history from America's West Coast.\"\n",
    "# ```\n",
    "\n",
    "# ### JPL Mars Space Images - Featured Image\n",
    "\n",
    "# * Visit the url for JPL Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "\n",
    "# * Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`.\n",
    "\n",
    "# * Make sure to find the image url to the full size `.jpg` image.\n",
    "\n",
    "# * Make sure to save a complete url string for this image.\n",
    "\n",
    "# ```python\n",
    "# # Example:\n",
    "# featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16225_hires.jpg'\n",
    "# ```\n",
    "\n",
    "# ### Mars Weather\n",
    "\n",
    "# * Visit the Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called `mars_weather`.\n",
    "\n",
    "# ```python\n",
    "# # Example:\n",
    "# mars_weather = 'Sol 1801 (Aug 30, 2017), Sunny, high -21C/-5F, low -80C/-112F, pressure at 8.82 hPa, daylight 06:09-17:55'\n",
    "# ```\n",
    "\n",
    "# ### Mars Facts\n",
    "\n",
    "# * Visit the Mars Facts webpage [here](http://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "# * Use Pandas to convert the data to a HTML table string.\n",
    "\n",
    "# ### Mars Hemispheres\n",
    "\n",
    "# * Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "\n",
    "# * You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "# * Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "\n",
    "# * Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "# ```python\n",
    "# # Example:\n",
    "# hemisphere_image_urls = [\n",
    "#     {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "# ]\n",
    "# ```\n",
    "\n",
    "# - - -\n",
    "\n",
    "# ## Step 2 - MongoDB and Flask Application\n",
    "\n",
    "# Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "# * Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "# * Next, create a route called `/scrape` that will import your `scrape_mars.py` script and call your `scrape` function.\n",
    "\n",
    "#   * Store the return value in Mongo as a Python dictionary.\n",
    "\n",
    "# * Create a root route `/` that will query your Mongo database and pass the mars data into an HTML template to display the data.\n",
    "\n",
    "# * Create a template HTML file called `index.html` that will take the mars data dictionary and display all of the data in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design.\n",
    "\n",
    "# ![final_app_part1.png](Images/final_app_part1.png)\n",
    "# ![final_app_part2.png](Images/final_app_part2.png)\n",
    "\n",
    "# - - -\n",
    "\n",
    "# ## Hints\n",
    "\n",
    "# * Use Splinter to navigate the sites when needed and BeautifulSoup to help find and parse out the necessary data.\n",
    "\n",
    "# * Use Pymongo for CRUD applications for your database. For this homework, you can simply overwrite the existing document each time the `/scrape` url is visited and new data is obtained.\n",
    "\n",
    "# * Use Bootstrap to structure your HTML template.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Hamid\", \"Jawad\", \"Ebad\", \"James\"]\n",
    "\n",
    "location = [\"United States\", \"India\", \"Pakistan\"]\n",
    "\n",
    "jawad = \"India\"\n",
    "hamid = \"Pakistan\"\n",
    "\n",
    "bank_balance = 500\n",
    "\n",
    "jawads_balance = 0\n",
    "hamids_balance = 5000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5000030\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    if name == \"Jawad\":\n",
    "        jawads_balance = jawads_balance + 10\n",
    "        hamids_balance =  hamids_balance + jawads_balance * 3\n",
    "        print(jawads_balance, hamids_balance)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete your initial scraping using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter.\n",
    "\n",
    "from time import sleep\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from splinter import Browser\n",
    "import pymongo\n",
    "import selenium\n",
    "\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "espn_url = \"http://www.espn.com/nba/statistics/player/_/stat/scoring-per-game/sort/avgPoints\"\n",
    "\n",
    "browser.visit(espn_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0][0][0]\n",
    "\n",
    "for col in tables:\n",
    "    cols = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(espn_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_list = {}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = tables[0][0]\n",
    "\n",
    "\n",
    "size = len(rank) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, size):\n",
    "    rk = rank[i]\n",
    "    rank_list.append(rk)\n",
    "   \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_list = []\n",
    "\n",
    "players = tables[0][1]\n",
    "\n",
    "for player in players:\n",
    "    if player != \"PLAYER\":\n",
    "        player_list.append(player)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later.\n",
    "\n",
    "nasa_web = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(nasa_web)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.body.prettify())\n",
    "html_body = soup.body\n",
    "\n",
    "\n",
    "# collect the latest News Title\n",
    "slides = soup.find_all('li', class_=\"slide\")\n",
    "\n",
    "for slide in slides:\n",
    "    date = slide.find('div', class_='list_date').text\n",
    "    news_title = slide.find('div', class_='content_title').text\n",
    "    paragraph = slide.find('div', class_='article_teaser_body').text\n",
    "    \n",
    "    \n",
    "    print(date)\n",
    "    print(news_title)\n",
    "    \n",
    "    print(paragraph)\n",
    "    print('----'*4)\n",
    "    \n",
    "\n",
    "# slides.find_all('div', class_='list_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### JPL Mars Space Images - Featured Image\n",
    "\n",
    "# * Visit the url for JPL Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "# \n",
    "mars_images = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(mars_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`.\n",
    "##  htmlcode for button.\n",
    "#---> <a class='button' href style='display: inline-block;'> More</a> \n",
    "\n",
    "# * Make sure to find the image url to the full size `.jpg` image.\n",
    "\n",
    "\n",
    "# h3 = \"class=release_date\"\n",
    "#  date = image.find('time')['datetime']\n",
    "\n",
    "\n",
    "\n",
    "# * Make sure to save a complete url string for this image.\n",
    "html_3 = browser.html\n",
    "soup_3 = BeautifulSoup(html_3,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = soup_3.find_all('li', class_='slide')\n",
    "    \n",
    "x = 0 \n",
    "    \n",
    "for image in images:\n",
    "    if x <= 20:\n",
    "        \n",
    "        dates = image.find('h3', class_='release_date').text\n",
    "        link = image.a['data-fancybox-href']\n",
    "        sleep(1)\n",
    "        \n",
    "        x = x +1\n",
    "        print(x)\n",
    "        print('--'*8)\n",
    "        print(dates)\n",
    "        print('image url:', 'https://www.jpl.nasa.gov' + link)\n",
    "        \n",
    "#         browser.click_link_by_partial_text('MORE')\n",
    "        \n",
    "        \n",
    "\n",
    "       \n",
    "    \n",
    "  \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     post = {\n",
    "#         'date': dates,\n",
    "#         'link': link,\n",
    "#     }\n",
    "    \n",
    "#     collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Mars Weather\n",
    "\n",
    "# * Visit the Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called `mars_weather`.\n",
    "\n",
    "mars_twitter = \"https://twitter.com/marswxreport?lang=en\"\n",
    "\n",
    "browser.visit(mars_twitter)\n",
    "html_2 = browser.html\n",
    "soup_2 = BeautifulSoup(html_2,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = soup_2.find_all('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\")\n",
    "\n",
    "count = 1\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_post = tweet.text\n",
    "    \n",
    "    if tweet.text[:3] == \"Sol\":\n",
    "        print('---------')\n",
    "        print(tweet_post.replace(', ','\\n'))\n",
    "\n",
    "    \n",
    "   \n",
    "          \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Mars Facts\n",
    "\n",
    "# * Visit the Mars Facts webpage [here](http://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "# * Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts_url = \"http://space-facts.com/mars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(mars_facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tables[0]\n",
    "df.columns = ['Description', 'Value']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_table = df.to_html('table.html')\n",
    "\n",
    "# html_table.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Mars Hemispheres\n",
    "\n",
    "# * Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "\n",
    "# * You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "# * Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "\n",
    "# * Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "# ```python\n",
    "# # Example:\n",
    "# hemisphere_image_urls = [\n",
    "#     {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "#     {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astrogeology_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(astrogeology_url)\n",
    "\n",
    "astro_html = browser.html\n",
    "astro = BeautifulSoup(astro_html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.find_by_css('.thumb')[0].click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.click_link_by_text('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(astrogeology_url)\n",
    "sleep(2)\n",
    "browser.find_by_css('.thumb')[1].click()\n",
    "browser.click_link_by_text(\"Sample\")\n",
    "browser.url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(astrogeology_url)\n",
    "browser.find_by_css('.thumb')[2].click()\n",
    "browser.click_link_by_text(\"Sample\")\n",
    "browser.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(astrogeology_url)\n",
    "browser.find_by_css('.thumb')[3].click()\n",
    "browser.click_link_by_text(\"Sample\")\n",
    "browser.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemispheres = astro.find('div', class_=\"collapsible results\")\n",
    "hem = hemispheres.find_all('div', class_='item')\n",
    "\n",
    "for h in hem:\n",
    "    hem_ref = h.a['href']\n",
    "    bb = browser.find_by_css('a.product-item')\n",
    "    bb[h].click()\n",
    "    sleep(2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "browser = init_browser()\n",
    "browser.visit(url5)\n",
    "\n",
    "first = browser.find_by_tag('h3')[0].text\n",
    "second = browser.find_by_tag('h3')[1].text\n",
    "third = browser.find_by_tag('h3')[2].text\n",
    "fourth = browser.find_by_tag('h3')[3].text\n",
    "\n",
    "browser.find_by_css('.thumb')[0].click()\n",
    "first_img = browser.find_by_text('Sample')['href']\n",
    "browser.back()\n",
    "\n",
    "browser.find_by_css('.thumb')[1].click()\n",
    "second_img = browser.find_by_text('Sample')['href']\n",
    "browser.back()\n",
    "\n",
    "browser.find_by_css('.thumb')[2].click()\n",
    "third_img = browser.find_by_text('Sample')['href']\n",
    "browser.back()\n",
    "\n",
    "browser.find_by_css('.thumb')[3].click()\n",
    "fourth_img = browser.find_by_text('Sample')['href']\n",
    "\n",
    "hemisphere_image_urls = [\n",
    "    {'title': first, 'img_url': first_img},\n",
    "    {'title': second, 'img_url': second_img},\n",
    "    {'title': third, 'img_url': third_img},\n",
    "    {'title': fourth, 'img_url': fourth_img}\n",
    "]\n",
    "\n",
    "print(hemisphere_image_urls)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 2 - MongoDB and Flask Application\n",
    "\n",
    "# Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "conn='mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# * Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "db = client.mars_db\n",
    "collection = db.items\n",
    "\n",
    "# * Next, create a route called `/scrape` that will import your `scrape_mars.py` script and call your `scrape` function.\n",
    "\n",
    "#   * Store the return value in Mongo as a Python dictionary.\n",
    "\n",
    "# * Create a root route `/` that will query your Mongo database and pass the mars data into an HTML template to display the data.\n",
    "\n",
    "# * Create a template HTML file called `index.html` that will take the mars data dictionary and display all of the data in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design.\n",
    "\n",
    "# ![final_app_part1.png](Images/final_app_part1.png)\n",
    "# ![final_app_part2.png](Images/final_app_part2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
